# -*- coding: utf-8 -*-
"""Fake news Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QISrSDUcK-xNZtxSJ6hnn16NMg1lhJST
"""

# import library
import nltk
nltk.download('punkt')
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df_true = pd.read_csv('/content/True news.csv')
df_fake = pd.read_csv('/content/Fake news.csv')

df_true

df_fake

df_true.isnull().sum()

df_fake.isnull().sum()

df_true.info()

df_fake.info()

df_true['isfake'] = 0
df_true.head()

df_fake['isfake'] = 1
df_fake.head()

# concatinate the both dataset into one dataset with continious indexing
df = pd.concat([df_true,df_fake]).reset_index(drop=True)
df

df.drop(columns=['date'],inplace=True)

df['original'] = df['title'] + ' ' + df['text']
df.head()

df['original'][0]

# Data cleaning
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from','subject','re','edu','use']) # words that you want to remove

stop_words

# cleaning the dataset
!pip install --upgrade --force-reinstall numpy  # Reinstall NumPy
!pip install --upgrade --force-reinstall gensim # Reinstall Gensim after NumPy
import gensim
#from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS

def preprocess(text):
  result = []
  for taken in gensim.utils.simple_preprocess(text):
    if taken not in gensim.parsing.preprocessing.STOPWORDS and len(taken) > 3 and taken not in stop_words:
      result.append(taken)
  return result

df['clean'] = df['original'].apply(preprocess)

df['clean'] = df['original'].apply(preprocess)

df['original'][0]

# cleaned Data from the original News data
print(df['clean'][0])

list_of_words = []
for i in df['clean']:
  for j in i:
    list_of_words.append(j)

list_of_words

#total no. of words
len(list_of_words)

#list of unique words
total_words = set(list_of_words)
len(total_words)

df['clean_joined'] = df['clean'].apply(lambda x: " ".join(x))
df

df['clean_joined'][0]

#visualizing the subject of dataset
plt.figure(figsize = (8,8))
sns.countplot(y = 'subject',data = df)

# visualizing the numbers of fakenews
plt.figure(figsize = (8,8))
sns.countplot(y = 'isfake',data = df)

from wordcloud import WordCloud
plt.figure(figsize=(20,20))
# Create an instance of WordCloud
wc = WordCloud(max_words = 2000,width = 1600,height = 800,stopwords = stop_words).generate(" ".join(df[df.isfake == 1].clean_joined))
plt.imshow(wc,interpolation='bilinear') # Also corrected interpolation to 'bilinear'

from wordcloud import WordCloud
plt.figure(figsize=(20,20))
# Create an instance of WordCloud
wc = WordCloud(max_words = 2000,width = 1600,height = 800,stopwords = stop_words).generate(" ".join(df[df.isfake == 0].clean_joined))
plt.imshow(wc,interpolation='bilinear') # Also corrected interpolation to 'bilinear'

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(df.clean_joined,df.isfake,test_size=0.2,random_state=2)

# word ambeding
from nltk.tokenize import word_tokenize
maxlen = -1
for doc in df.clean_joined:
  tokens = word_tokenize(doc)
  if maxlen < len(tokens):
    maxlen = len(tokens)
print('The maximum number of words in any document =', maxlen)

from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words = total_words)
train_sequences = tokenizer.texts_to_sequences(x_train)
test_sequences = tokenizer.texts_to_sequences(x_test)

len(train_sequences)

len(test_sequences)

print('The encodeing for documnet\n',df.clean_joined[0],'\n',train_sequences[0])

from os import truncate
from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_train = pad_sequences(train_sequences,maxlen=maxlen,padding = 'post',truncating = 'post')
padded_test = pad_sequences(test_sequences,maxlen=maxlen,padding = 'post',truncating = 'post')

for i,doc in enumerate(padded_train[:2]):
  print('The padded encoding for documnet',i+1,'is')
  print(doc)

import tensorflow.keras.models as Sequential
from tensorflow.keras.layers import Dense,Embedding,LSTM,Bidirectional
model = Sequential.Sequential()
model.add(Embedding(total_words,output_dim=128))
model.add(Bidirectional(LSTM(128)))
model.add(Dense(128,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])

total_words = len(total_words)

y_train = np.asarray(y_train)
y_test = np.asarray(y_test)

# Before creating the model
total_words = len(set(list_of_words))  # Ensure total_words is an integer

model.fit(padded_train,y_train,batch_size =64,epochs=2,validation_split=0.1)

# Before creating the model
total_words = len(set(list_of_words))  # Ensure total_words is an integer

pred = model.predict(padded_test)
prediction = []
for i in range(len(pred)):
  if pred[i].item() > 0.5:
    prediction.append(1)
  else:
    prediction.append(0)
from sklearn.matrics import accuracy_score
accuracy = accuracy_score(list(y_test),prediction)
print(accuracy)

from sklearn.matrics import confusion_matrix
cm = confusion_matrix(list(y_test),prediction)
plt.figure(figsize=(25,25))
sns.heatmap(cm,annot=True)